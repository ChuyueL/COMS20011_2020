{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import Normal, Categorical, Bernoulli\n",
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual\n",
    "mpl.rcParams['figure.max_open_warning'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\Categorical}{{\\rm Categorical}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}[1][]{{\\rm P}_{#1}\\!\\b}\n",
    "\\newcommand{\\Q}[1][]{{\\rm Q}_{#1}\\!\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}[1][]{\\mathbb{E}_{#1}\\!\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\k}{\\mathbf{k}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "\\newcommand{\\p}{\\mathbf{p}}\n",
    "\\newcommand{\\q}{\\mathbf{q}}\n",
    "\\newcommand{\\l}{\\boldsymbol{\\ell}}\n",
    "\\DeclareMathOperator{\\softmax}{softmax}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\norm}{\\bracket{\\lVert}{\\rVert}}\n",
    "\\newcommand{\\Dkl}[2]{D_\\text{KL} \\left( #1 \\middle\\Vert #2 \\right)}\n",
    "$$\n",
    "\n",
    "<h1> Part 5: Unsupervised learning and clustering </h1>\n",
    "\n",
    "<h2> Clustering is not classification... </h2>\n",
    "<h2>...and unsupervised learning is not supervised learning </h2>\n",
    "Clustering is a type of unsupervised learning, which is very, very different from the supervised learning you have seen in my slides so far.\n",
    "\n",
    "In supervised learning, we have a list of input, `x`, output, `y`, pairs as data, and the idea is to learn a function that maps from a new input test point, to a distribution over the corresonding `Y`\n",
    "- learning algo that returns a func that maps from x to some guess about y\n",
    "\n",
    "```\n",
    "#### Supervised learning\n",
    "x : X # Input\n",
    "y : Y # Output\n",
    "    [(X, Y)] -> (X -> Y) \n",
    "```\n",
    "\n",
    "In unsupervised learning/clustering, we only have a list of inputs, and the goal is to compute a (distribution over) a latent variable that somehow summarises the structure of the inputs.\n",
    "- only a list of x's\n",
    "- no set of x,y pairs to learn\n",
    "- take x's and tell us something about the structure of the x's\n",
    "- we'll be focusing on clustering, so the structure we're interested in is the clusters\n",
    "- Zs are cluster indices\n",
    "- end up with something describing each x point we had as input\n",
    "\n",
    "```\n",
    "#### Unsupervised learning\n",
    "x : X # Input\n",
    "z : Z # Latent\n",
    "    [X] -> [Z]\n",
    "```\n",
    "\n",
    "<h2> First example </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e580ce6267489d988c6f9029b2f323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "Z = t.cat([\n",
    "    t.zeros(50).int(),\n",
    "    t.ones(50).int()\n",
    "])\n",
    "\n",
    "#fig = plot.figure()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(7,3), sharey=True)\n",
    "axs[0].set_xlim(-2, 2)\n",
    "axs[1].set_xlim(-2, 2)\n",
    "axs[2].set_xlim(-2, 2)\n",
    "axs[0].set_ylim(-2, 2)\n",
    "axs[0].scatter(X[:, 0], X[:, 1])\n",
    "axs[1].scatter(X[:, 0], X[:, 1], c=Z);\n",
    "axs[2].scatter(X[:, 0], X[:, 1], c=-Z);\n",
    "axs[0].set_title(\"original data, no labels\")\n",
    "axs[1].set_title(\"clustered data\")\n",
    "axs[2].set_title(\"equiv. clustering\");\n",
    "\n",
    "'''\n",
    "x is a 2D vector\n",
    "\n",
    "actual cluster assignments are arbitrary\n",
    "- Could swap the labels\n",
    "\n",
    "Data is only x locations, nothing to say that e.g. pts on the left are\n",
    "more like a 1\n",
    "\n",
    "All we can extract is whether data points are in the same cluster\n",
    "\n",
    "In both clusterings, whether 2 points are in same/diff cluster is maintained\n",
    "\n",
    "Equivalent clusterings\n",
    "- Distinction w/ classification\n",
    "- In classification, the 2 plots wouldn't be equivalent\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> A classification problem where we need to ignore clusters </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8eb7fa58f1417d913f016653c87536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = Bernoulli(t.sigmoid(100*X[:, 1])).sample()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_xlabel(\"height\")\n",
    "ax.set_ylabel(\"A-level scores\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y);\n",
    "\n",
    "'''\n",
    "1 attribute on x-axis, which is strongly bimodal (two big lumps)\n",
    "- this is exaggerated\n",
    "- in reality will be slightly bimodal (men and women have different heights)\n",
    "\n",
    "But if there are enough attributes, often many protected characteristics (e.g. gender)\n",
    "will separate or cluster\n",
    "\n",
    "classification problem\n",
    "- yellow and purple are whether we should admit person to UoB, for e.g.\n",
    "- this depends entirely on A level score and not at all on height\n",
    "- so if class boundary aligns with cluster boundary, this will be really bad\n",
    "\n",
    "But A level scores aren't bimodal. People just happen to cluster along irrelevant axis\n",
    "\n",
    "So when in classification setting, need to make sure to ignore clustering structure\n",
    "\n",
    "But in some cases (e.g. clustering) we are just interested in where the clusters lie\n",
    "\n",
    "It becomes very dangerous to try to do clustering first, then classification based on those\n",
    "clusters\n",
    "e.g. Cluster data then say that within a cluster, everyone should be admitted/rejected\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Simplest clustering algorithm: K-means </h2>\n",
    "\n",
    "We start by introducing cluster centers,\n",
    "\\begin{align}\n",
    "  \\m_z &= \\text{cluster center, for cluster indexed $z$}\\\\\n",
    "\\end{align}\n",
    "and integer-valued cluster assignments, $z_i$, describing the cluster associated with the $i$th datapoint,\n",
    "\\begin{align}\n",
    "  z_i &= \\text{integer cluster index for each datapoint, indexed $i$}\\\\\n",
    "\\end{align}\n",
    "\n",
    "The objective is find a bunch of cluster centers such that all datapoints are close to a cluster center.  Formally, we minimise the squared distance between each datapoint and its assigned cluster center,\n",
    "- an objective is a function/number we need to minimise (or maximise) for ML algos\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(\\z, \\m) &= \\sum_i \\norm{\\x_i - \\m_{z_i}}^2\n",
    "\\end{align}\n",
    "\n",
    "- this is the objective function\n",
    "- sum of squared distances between x (actual data point) and its assigned cluster centre\n",
    "\n",
    "To optimize this algorithm, we use coordinate descent (i.e. we alternate between optimizing $\\z$ and $\\m$).\n",
    "- not possible to use the methods we've used previously (gradient, or gradient descent) as the z_i's are integers so doesn't make sense to differentiate wrt those integers\n",
    "- fix z then find the best mu for that fixed z, then fix mu and find the best z for that fixed mu\n",
    "- finds an optimum of the global loss function\n",
    "\n",
    "First, we assign each datapoint to the nearest cluster,\n",
    "\n",
    "\\begin{align}\n",
    "  z_i &\\leftarrow \\argmin_{z\\in\\{1...Z\\}} \\norm{\\x_i - \\m_z}^2\n",
    "\\end{align}\n",
    "\n",
    "- same as minimising the loss (objective) for fixed mu wrt z's\n",
    "- compute distance between data point and every cluster centre\n",
    "- choose z_i to be the index of the closest cluster centre\n",
    "\n",
    "Then, we put the cluster centers at the mean of the assigned datapoints,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m_z &\\leftarrow \\frac{1}{\\sum_i \\delta_{z, z_i}} \\sum_{i} \\delta_{z, z_i} \\x_i\n",
    "\\end{align}\n",
    "\n",
    "- optimise cluster centres for fixed cluster assignments\n",
    "- we should take gradient of loss wrt mu \n",
    "- we'll shortcut to the solution here though\n",
    "- take all data pts in cluster and take the mean of these - this is the new cluster centre\n",
    "\n",
    "remember that $z$ is an index, whereas $z_i$ is the actual cluster assignment for datapoint $i$.\n",
    "\n",
    "\\begin{align}\n",
    "  \\delta_{z, z_i} = \n",
    "  \\begin{cases}\n",
    "    1 &\\text{if $z=z_i$, i.e. datapoint $i$ is currently assigned to the $z$th cluster}\\\\\n",
    "    0 &\\text{if $z\\neq z_i$, i.e. datapoint $i$ is not in the $z$th cluster}\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "- kroenecker delta\n",
    "- z is the cluster currently being considered\n",
    "- 1 if data point i is in the cluster we're considering\n",
    "\n",
    "so,\n",
    "\n",
    "\\begin{align}\n",
    "  \\sum_i \\delta_{z, z_i} &= \\text{number of datapoints in cluster $z$}\\\\\n",
    "  \\sum_i \\x_i \\delta_{z, z_i} &= \\text{sum of all the datapoints in cluster $z$}\n",
    "\\end{align}\n",
    "\n",
    "- the ratio of these is the mean of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50151f042a74bc592e7eed37cc366da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# N == number of datapoints\n",
    "# K == number of clusters\n",
    "# D == dimension of the data vectors (usually 2 in our examples)\n",
    "#X.shape  == (N, 1, D) # Data\n",
    "#mu.shape ==    (K, D) # Cluster-centers\n",
    "#q.shape  == (N, K, 1) # One-hot representation of the cluster-assignments for each datapoint\n",
    "#z.shape  == (N,)      # Cluster index for each datapoint\n",
    "\n",
    "t.manual_seed(0)\n",
    "\n",
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5,\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "def kmeans(X, K):\n",
    "    N, D = X.shape\n",
    "    X = X[:, None, :]\n",
    "    mu = t.randn(K, D)\n",
    "    while True:\n",
    "        sd = ((X - mu)**2).sum(-1) # sd.shape = (N, K)\n",
    "        z = t.argmin(sd, 1)        # z.shape  = (N)\n",
    "        q = t.zeros(N, K, 1)\n",
    "        q[t.arange(N), z, 0] = 1.\n",
    "        print(f\"loss = {loss(X, z, mu).item()}\")\n",
    "        plot(X, z, mu)\n",
    "        yield None\n",
    "        \n",
    "        mu = (q * X).sum(0) / q.sum(0)\n",
    "        print(f\"loss = {loss(X, z, mu).item()}\")\n",
    "        plot(X, z, mu)\n",
    "        yield None\n",
    "        \n",
    "def loss(X, z, mu):\n",
    "    return ((X - mu[z, :][:, None, :])**2).mean()\n",
    "    \n",
    "def plot(X, z, mu):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X[:, 0, 0], X[:, 0, 1], c=z);\n",
    "    ax.scatter(mu[:, 0], mu[:, 1], s=100, c='r', label=\"cluster centers\")\n",
    "    ax.legend()\n",
    "    \n",
    "kmeans_iter = iter(kmeans(X, 3))\n",
    "def kmeans_call():\n",
    "    next(kmeans_iter)\n",
    "interact_manual(kmeans_call);\n",
    "\n",
    "#Change marker type\n",
    "\n",
    "'''\n",
    "don't have to understand this code\n",
    "\n",
    "3 cluster centres\n",
    "3 clusters of underlying data\n",
    "\n",
    "cluster assignments in diff colours\n",
    "\n",
    "at start, cluster assignment at the top has very few data points associated with it\n",
    "\n",
    "then can update means (m step) and cluster assignments using the process\n",
    "\n",
    "Eventually, cluster centres settle down \n",
    "- Nothing changes if do more iterations and loss remains static\n",
    "- Because as soon as the cluster assignments stop changing, the updates of the \n",
    "cluster centres also stop changing so get exact convergence to a static fixed point\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one strange property of this algorithm is that it assumes \"hard\" cluster assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Soft clustering and the Gaussian Mixture Model (GMM) </h2>\n",
    "\n",
    "In general, the goal of unsupervised learning isn't just to assign data points to a cluster. It's to model the structure of the underlying data. There are lots of potential structures and lots of potential models to fit to those structures.\n",
    "- If we do fit these models, tend to get more flexibility and power than if we just use K-means.\n",
    "\n",
    "The goal of the GMM, is really just to model the density of the data we use the latent variables to give us a better model of the data.\n",
    "\n",
    "For instance, consider fitting a MvNormal to the following data,\n",
    "- multivariate gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logPx = -132.896728515625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5542ddfb052b478880f2395851439b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "N = X.shape[0]\n",
    "mu = X.sum(0)/N\n",
    "cov = ((X - mu).T @ (X-mu))/N\n",
    "mvn = MvNormal(mu, cov)\n",
    "\n",
    "logPx = mvn.log_prob(X).sum()\n",
    "print(f\"logPx = {logPx}\")\n",
    "\n",
    "P = 100\n",
    "x0s = t.linspace(-2, 2, P)[:, None].expand(P, P)\n",
    "x1s = x0s.T\n",
    "xs  = t.stack([x0s.reshape(-1), x1s.reshape(-1)], 1)\n",
    "ps = mvn.log_prob(xs).exp().view(P, P)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.scatter(X[:, 0], X[:, 1]);\n",
    "ax.contour(x0s, x1s, ps);\n",
    "\n",
    "'''\n",
    "Quite a bad fit\n",
    "- mode of the prob density is in the middle but there are actually two\n",
    "modes at either side\n",
    "\n",
    "Can we get a more powerful probabilistic model that allows us to capture the\n",
    "multimodal (mixture model) type structure\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get a better model for this kind of data?  We use a GMM.\n",
    "\n",
    "The simplest form of a GMM is a density, with $K$ different Gaussians, in different locations, $\\m_k$, and with different covariance matrices, $\\S_k$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_i} &= \\sum_{z=1}^Z p_z \\N{\\x_i; \\m_z, \\S_z}.\n",
    "\\end{align}\n",
    "\n",
    "- sum of multiple gaussian bumps\n",
    "- prob for any data point is a sum over different gaussians.\n",
    "- these gaussians are indexed with z\n",
    "- means and covariances are different for the different clusters z\n",
    "- looks like normal gaussian bumps\n",
    "- could maximise the log likelihood to find the optimal setting of p's, mu's and sigma's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this doesn't give us a way to extract any notion of assignment of a datapoint to a mixture component.\n",
    "- we wanted to give a cluster assignment for each data point\n",
    "- this model just says the prob density of a data point given the ps, mus and sigmas as parameters\n",
    "\n",
    "We therefore write this distribution in terms of a latent variable,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_i} &= \\sum_{z_i=1}^Z \\P{\\x_i| z_i} \\P{z_i}\n",
    "\\end{align}\n",
    "\n",
    "- the latent variable z is the cluster assignment\n",
    "- Z is the number of clusters \n",
    "\n",
    "where, $\\P{z_i}$ is a Categorical (i.e. a distribution over integers from $1$ to $Z$), and represents the mixture component for the $i$th data point, and $\\P{x_i| z_i}$ is a _single_ Gaussian, (corresponding to the $z_i$th mixture component),\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{z_i} &= \\Categorical{z_i; \\p} = p_{z_i}\\\\\n",
    "  \\P{\\x_i| z_i} &= \\N{\\x_i; \\m_{z_i}, \\S_{z_i}}.\n",
    "\\end{align}\n",
    "\n",
    "- use a uniform prior over z_i (so probability is 1/Z for any possible setting)\n",
    "- could also be the p_z above, depending on how you set it up\n",
    "- this is a simplification. really we would take the distribution of z_i is that element of p\n",
    "\n",
    "The substituting in the values of $\\P{z_i}$ and $\\P{x_i| z_i}$, we get the same expression as above, except use $z_i$ as the summation index, instead of $z$\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_i} &= \\sum_{z_i=1}^Z \\P{\\x_i| z_i} \\P{z_i} = \\sum_{z_i=1}^Z p_{z_i} \\N{\\x_i; \\m_{z_i}, \\S_{z_i}}.\n",
    "\\end{align}\n",
    "\n",
    "Now we have a model where these cluster assignment appear and is also probabilistic (could learn the sigmas, for example).\n",
    "- this also allows us to have soft cluster assignments\n",
    "- previously, had to say a data point belongs entirely to one cluster or another \n",
    "- now we're able to say that maybe a data point belongs to one cluster, or maybe another one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use Bayes theorem to give a posterior distribution over $z_i$ (to do soft clustering),\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{z_i| \\x_i} &\\propto \\P{\\x_i| z_i} \\P{z_i}\n",
    "\\end{align}\n",
    "\n",
    "- prior is often uniform\n",
    "- so if the data point is very likely under that particular setting of the cluster, then it's going to be likely that that's actually the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggests an algorithm (EM) similar to K-means: first compute the posterior over $z_i$ (update the soft cluster assignments) (E-step), then update the parameters of the Gaussians, $\\m_z$, using a mean weighted by the posterior (M-step) because data points aren't entirely assigned to one class or the other.\n",
    "\n",
    "In particular, the **E-step** updates the posterior over $z_i$ for a given $x_i$, and \"saves\" the posterior into $\\q$.  The denominator normalizes the distribution so that it sums to 1.\n",
    "\\begin{align}\n",
    "  q_{i; z} &\\leftarrow \\P{z_i=z| x_i} = \\frac{\\P{\\x_i| z_i=z} \\P{z_i=z}}{\\sum_{z'} \\P{\\x_i| z_i=z'} \\P{z_i=z'}}\n",
    "\\end{align}\n",
    "\n",
    "- q is like a matrix: has 2 indices: 1 for the i-th data point and 1 for the z-th cluster\n",
    "- posterior probability that the i-th data point was in cluster z, given the set of mean locations\n",
    "\n",
    "and the **M-step**, is a weighted average of $\\x_i$'s, with the weights corresponding to the degree to which that datapoint is assigned to that cluster,\n",
    "\\begin{align}\n",
    "  \\m_z &\\leftarrow \\frac{\\sum_i \\x_i q_{i, z}}{\\sum_i q_{i, z}}\n",
    "\\end{align}\n",
    "\n",
    "- take weighted mean of the x's that belong to that cluster assignment\n",
    "- when q_i,z is large, data pt prob belongs to cluster z so should move the mean location towards that data point\n",
    "- if we're very certain, q_i,z is 0 everywhere apart from the \"correct\" cluster assignment\n",
    "\n",
    "In the case where we are very sure about the cluster assignments, the posteriors become equal to the hard-assignment case, because $\\P{z_i=z| x_i}$ is $1$ for the \"right\" assignment and $0$ otherwise,\n",
    "\n",
    "\\begin{align}\n",
    "  q_{i; z} &= \\P{z_i=z| x_i} = \\delta_{z_i, z}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54a87e0576f4b15ac733d0b3f553bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.manual_seed(0)\n",
    "\n",
    "X = t.cat([\n",
    "    t.randn(50, 2)/3 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/3 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "def gmm_em(X, K):\n",
    "    N, D = X.shape\n",
    "    X = X[:, None, :]\n",
    "    mu  = t.randn(K, D)\n",
    "    cov = 0.5*t.eye(D, D).expand(K, -1, -1)\n",
    "    while True:\n",
    "        # unnormalized posterior probability\n",
    "        q = MvNormal(mu, cov).log_prob(X).exp()\n",
    "        # normalized posterior probability\n",
    "        q = q / q.sum(1, keepdim=True)\n",
    "        # expand out last such that q.shape = (N, K, 1)\n",
    "        q = q[:, :,  None]\n",
    "        plot_gmm(X, q, mu, cov)\n",
    "        yield None\n",
    "        \n",
    "        #weighted mean\n",
    "        mu = (q * X).sum(0) / q.sum(0)\n",
    "        plot_gmm(X, q, mu, cov)\n",
    "        yield None\n",
    "\n",
    "        \n",
    "def plot_gmm(X, q, mu, cov):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X[:, 0, 0], X[:, 0, 1], c=q[:, 0, 0]);\n",
    "    ax.scatter(mu[:, 0], mu[:, 1], s=100, c='r', label=\"cluster centers\")\n",
    "    ax.legend()\n",
    "\n",
    "    \n",
    "gmm_em_iter = iter(gmm_em(X, 2))\n",
    "def gmm_em_call():\n",
    "    next(gmm_em_iter)\n",
    "interact_manual(gmm_em_call);\n",
    "\n",
    "#Change marker type\n",
    "\n",
    "'''\n",
    "cluster assignments of data pts are now soft\n",
    "points in middle are partially assigned to both clusters (roughly equidistant between both clusters)\n",
    "\n",
    "as we'd expect, most data points are mostly ascribed to one cluster centre\n",
    "some degree of mixing for points that lie in the middle\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> What is the objective function for EM? [Non-examinable] </h2>\n",
    "\n",
    "For all ML approaches, we need a loss function.\n",
    "- often this is maximum likehood - maximise the log likelihood\n",
    "- or minimise some loss (K means clustering)\n",
    "\n",
    "Turns out that this is a very subtle question.\n",
    "\n",
    "We need to define an \"approximate posterior\", corresponding to the current cluster assignments,\n",
    "\n",
    "\\begin{align}\n",
    "  \\Q[\\q]{z_i} &= \\Categorical{z_i; \\q_{i}} = q_{i, z_i}\n",
    "\\end{align}\n",
    "\n",
    "We can write down the \"evidence lower-bound objective\" (ELBO).  Note that the \"model evidence\" is $\\log \\P{\\x}$, and the ELBO can be defined as,\n",
    "\n",
    "\\begin{align}\n",
    "  \\log \\P[\\m]{\\x} \\geq \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x}- \\Dkl{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}\n",
    "\\end{align}\n",
    "\n",
    "Where the ELBO is a lower-bound because the KL-divergence is non-negative,\n",
    "\n",
    "\\begin{align}\n",
    "  0 \\leq \\Dkl{\\Q[q]{\\z}}{\\P[\\m]{\\z| \\x}} = \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}}\n",
    "\\end{align}\n",
    "\n",
    "And the KL-divergence equals zero when the approximate posterior equals the true posterior, $\\Q{\\z} = \\P{\\z| \\x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Expectation (E) step </h4>\n",
    "\n",
    "Thus, the E-step consists of maximizing the ELBO wrt to the parameters of the approximate posterior, $\\q$.\n",
    "\n",
    "\\begin{align}\n",
    "  \\q \\leftarrow \\argmax_{\\q} \\L\\b{\\m, \\q}\n",
    "\\end{align}\n",
    "\n",
    "as $\\log \\P[\\m]{\\x}$ does not depend on $\\q$ this is equivalent to,\n",
    "\n",
    "\\begin{align}\n",
    "  \\q \\leftarrow \\argmax_{\\q} \\Dkl{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}} \n",
    "\\end{align}\n",
    "\n",
    "This KL-divergence is minimized by setting $\\q$ to the true posterior (i.e. the E-step that we saw above), concretely, we can achieve this by setting,\n",
    "\n",
    "\\begin{align}\n",
    "  q_{i, z} \\leftarrow \\P{z_i=z| \\x_i}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Maximization (M) step </h3>\n",
    "\n",
    "The goal is,\n",
    "\\begin{align}\n",
    "  \\m \\rightarrow \\argmax_{\\m} \\L\\b{\\m, \\q}\n",
    "\\end{align}\n",
    "\n",
    "Both terms in the previous form for the ELBO depend on $\\m$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x} + \\Dkl{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}\n",
    "\\end{align}\n",
    "\n",
    "so we need to rearrange to get a good update for $\\m$.\n",
    "\n",
    "In particular,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x} - \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\Q[\\q]{\\z}}{\\P[\\m]{\\z| \\x}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\log \\P[\\m]{\\x} + \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\P[\\m]{\\z| \\x}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\P[\\m]{\\x} + \\log \\frac{\\P[\\m]{\\z| \\x}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\P[\\m]{\\z| \\x} \\P[\\m]{\\x}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\frac{\\P[\\m]{\\x, \\z}}{\\Q[\\q]{\\z}}}\\\\\n",
    "  \\L\\b{\\m, \\q} &= \\E[{\\Q[\\q]{\\z}}]{\\log \\P[\\m]{\\x, \\z}} - \\E[{\\Q[\\q]{\\z}}]{\\Q[\\q]{\\z}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that the second term doesn't depend on $\\m$, so,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m \\rightarrow \\argmax_{\\m} \\E[{\\Q[\\q]{\\z}}]{\\log \\P[\\m]{\\x, \\z}}\n",
    "\\end{align}\n",
    "\n",
    "And this is just maximum-likelihood fitting of the parameters of the mixtures (here, the mixture-means), with datapoints weighted by $\\Q{\\z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
